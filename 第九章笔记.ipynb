{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 第九章 NPL 自然语言处理和分词"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "source": [
    "NPL （Natural Language Processing）自然语言处理。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 文本预处理\n",
    "首先我们要做的是预测文本的下一个单词，在建立神经网络模型之前，对于文本中的单词，预处理主要有以下步骤：\n",
    "    \n",
    "1. 将所有出现的单词编译成可编程变量，将其列到一个列表中（这个list被成为vocab）\n",
    "2. 给每个单词附上独特的编号\n",
    "3. 建立一个嵌入矩阵来放这些单词，每一行对应一个vocab\n",
    "4. 使用这些矩阵作为神经网络的第一层\n",
    "\n",
    "对于文本数据，首先我们要把要预测的文本变成一个长长的字符串，然后模型的自变量应该是这串长长字符串的第一个单词到最后第二个单词，因变量是第二个单词到最后一个单词。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- 标记化：将文本转换为单词（或字符或子字符串，取决于模型的粒度）列表\n",
    "- 数字化：列出出现的所有唯一词（词汇），并通过在词汇中查找其索引将每个单词转换为数字\n",
    "- 语言模型数据加载器的创建：fastai提供了一个“ LMDataLoader”类，该类自动处理创建因变量而该因变量偏离自变量一个标记的情况。它还处理一些重要的细节，例如，如何以因变量和自变量按需保持其结构的方式对训练数据进行混洗\n",
    "- 语言模型的创建：我们需要一种特殊的模型，该模型可以完成我们之前从未见过的事情：处理输入列表，可以任意大小。有很多方法可以做到这一点。在本章中，我们将使用“递归神经网络”（RNN）。我们将在<< chapter_nlp_dive >>中获得这些RNN的详细信息，但是现在，您可以将其视为另一个深度神经网络。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 令牌化\n",
    "\n",
    "在这个阶段主要处理的是如何将一个具有实际意义的词挑出来。遇到的问题会有很多，比如don’t是一个词还算是两个词呢？有些很长的化学名词和生物名词怎么办？如何处理中文和日语这样没有具体单词划分的语言？这些问题的解决要基于三个原则：\n",
    "\n",
    "- 基于单词的分割：在词和词之间的空格出进行分割，同时也要将有语义不同的地方分割出来，比如说dont会分成do nt,这个情况下标点符号一般是分割的地方。_\n",
    "- 基于子词的分割：根据最常出现的子字符串来进行分割，比如occasion可能会被分割成o c ca sion\n",
    "- 基于字母的分割：单个字母单个字母进行分割\n",
    "\n",
    "fastai没有做自己的分词器，但是做了一个接口可以调用不同的分词器\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Once again Mr. Costner has dragged out a movie for far longer than necessar'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "source": [
    "fastai默认的分词器是WorkTokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(#187) ['Once','again','Mr.','Costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','Aside','from','the','terrific','sea','rescue','sequences',',','of','which','there','are','very','few','I'...]\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "source": [
    "我们可以从结果中看出一些分词的规律，比如说it's被分成了it和‘s，逗号被分成了单独的符号，"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "source": [
    "再来看一个例子，我们可以看出分词器知道U.S.即使有个'.',但是分词器也知道这个是一个单词，同时分词器也把1.00当成一个单词而不是两个单词。\n",
    "\n",
    "同时fastai还有`Tokenizer`这个类，用来处理分词后的结果，使之可以被正常的使用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "source": [
    "上面的是分词化的结果，除了原来的句子之外，还有一些奇怪的标识符，比如xxbos，xxmaj等，这些标识符都有自己的作用，比如说xxbos这个标识符意味着这个是这段文档的开头，xxmaj说明后面的单词的首字母需要大写，同时我们可以发现所有的大写都被转成了小写字符，原因如果不处理大写和小写的单词，会被程序识别成两个单词，但是事实上这两个单词是一个单词，只是大小写形式不同罢了。下面是一些标识符的意思。\n",
    "\n",
    "- xxbos ::表示文本的开头 \n",
    "- xxmaj ::表示下一个单词以大写字母开头\n",
    "- xxunk ::表示下一个单词未知"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "所有的规则都可以在`defaults.text_proc_rules`被找到。\n",
    "\n",
    "\n",
    "- fix_html ::用可读的版本替换特殊的HTML字符。\n",
    "- replace_rep ：：用特殊的重复标记（xxrep）替换任何重复了3次或更多次的字符，然后重复该字符的次数，然后替换该字符；\n",
    "- replace_wrep ::用特殊的单词重复标记（xxwrep），将重复了3次以上的单词替换为特殊标记，重复该单词的次数，然后替换该单词；\n",
    "- spec_add_spaces ::在/和＃周围添加空格。\n",
    "- rm_useless_spaces ::删除所有重复的空格字符。\n",
    "- replace_all_copy ::小写所有大写字母的单词，并在其前面为所有大写字母（cap）添加一个特殊标记\n",
    "- replace_maj ::小写一个大写单词，并在其前面添加一个用于大写的特殊标记（xxmaj）\n",
    "- lowercase：:小写所有文本，并在开头（xxbos）添加一个特殊标记，"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "比如说这段文本"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "source": [
    "可以看到文档开头有xxbos的标识，'&copy'被转为了unicode，fast.ai的前面需要大写，所以有xxmaj的标识符，'xxrep','3','w',说明了www，INDEX被全部转为小写。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 子词化subword"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "对于某些没有特别明确单词概念的语言来说（比如中文和日语），subword是一个更好的分词方式。这个步骤一般有两步：\n",
    "- 找出文档中最常用的词群，将他们划为一个词\n",
    "- 将这些词令牌化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:40])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'▁O n ce ▁again ▁M r . ▁Co st n er ▁has ▁d ra g g ed ▁out ▁a ▁movie ▁for ▁far ▁long er ▁than ▁ ne ce s s ary . ▁A side ▁from ▁the ▁ ter ri f'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "txts = L(o.open(encoding='utf-8').read() for o in files[:2000])\n",
    "subword(1000)"
   ]
  },
  {
   "source": [
    "分词效果不太好，Once被分成了O n ce，again倒是正常的被分类了。如果使用更大的参数效果会更好一点"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'▁On ce ▁again ▁Mr . ▁Costner ▁has ▁dragged ▁out ▁a ▁movie ▁for ▁far ▁longer ▁than ▁necessary . ▁A side ▁from ▁the ▁terrific ▁sea ▁rescue ▁sequences , ▁of ▁which ▁there ▁are ▁very ▁few ▁I ▁just ▁did ▁not ▁care ▁about ▁any ▁of'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "subword(10000)"
   ]
  },
  {
   "source": [
    "虽然Once还是被分成了更加常见的on和ce，但是其他的单词的分词效果要好得多。参数越小，分词效果越过分，参数越大，分词效果越差。\n",
    "\n",
    "完成分词之后，接下来的工作就是将分词完成的单词数字化，以便程序进行计算。在进行这项工作之前，我们可以看一下完成分词之后的频率。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n"
     ]
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has'...]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"(#1968) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','it','i','in'...]\""
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "source": [
    "这是一个60000词（这个大小是默认大小）的词频分析，可以看到经过令牌化之后，标识符的数量是最多的，其次是the和一些标点符号，再次是一些英语中的介词，of，to之类的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorText([   2,    8,  349,  183,    8, 1176,   10,    8, 1177,   60, 1455,   62,   12,   25,   28,  189,  957,   93,  958,   10])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "source": [
    "经过序数化之后，我们就可以用一个数字来代替单词，2的意思是xxpad，8的意思是xxwrep，12的意思是句号.诸如此类。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " ## 创建批次"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "完成数字化之后，为了使用SGD算法，需要建立mini-batch，我们先来看一下原文的令牌化之后的语料。"
   ]
  },
  {
   "source": [
    ">  原文: In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> 令牌化：xxbos xxmaj in this chapter , we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj first we will look at the processing steps necessary to convert text into numbers and how to customize it . xxmaj by doing this , we 'll have another example of the preprocessor used in the data block xxup api . \\n xxmaj then we will study how we build a language model and train it for a while .\n",
    "\n",
    "我们发现令牌化之后这段话有90个词，我们采用一行为15个，总共6行的方式来建立这样一个小批次。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <tbody>\n    <tr>\n      <td>xxbos</td>\n      <td>xxmaj</td>\n      <td>in</td>\n      <td>this</td>\n      <td>chapter</td>\n      <td>,</td>\n      <td>we</td>\n      <td>will</td>\n      <td>go</td>\n      <td>back</td>\n      <td>over</td>\n      <td>the</td>\n      <td>example</td>\n      <td>of</td>\n      <td>classifying</td>\n    </tr>\n    <tr>\n      <td>movie</td>\n      <td>reviews</td>\n      <td>we</td>\n      <td>studied</td>\n      <td>in</td>\n      <td>chapter</td>\n      <td>1</td>\n      <td>and</td>\n      <td>dig</td>\n      <td>deeper</td>\n      <td>under</td>\n      <td>the</td>\n      <td>surface</td>\n      <td>.</td>\n      <td>xxmaj</td>\n    </tr>\n    <tr>\n      <td>first</td>\n      <td>we</td>\n      <td>will</td>\n      <td>look</td>\n      <td>at</td>\n      <td>the</td>\n      <td>processing</td>\n      <td>steps</td>\n      <td>necessary</td>\n      <td>to</td>\n      <td>convert</td>\n      <td>text</td>\n      <td>into</td>\n      <td>numbers</td>\n      <td>and</td>\n    </tr>\n    <tr>\n      <td>how</td>\n      <td>to</td>\n      <td>customize</td>\n      <td>it</td>\n      <td>.</td>\n      <td>xxmaj</td>\n      <td>by</td>\n      <td>doing</td>\n      <td>this</td>\n      <td>,</td>\n      <td>we</td>\n      <td>'ll</td>\n      <td>have</td>\n      <td>another</td>\n      <td>example</td>\n    </tr>\n    <tr>\n      <td>of</td>\n      <td>the</td>\n      <td>preprocessor</td>\n      <td>used</td>\n      <td>in</td>\n      <td>the</td>\n      <td>data</td>\n      <td>block</td>\n      <td>xxup</td>\n      <td>api</td>\n      <td>.</td>\n      <td>\\n</td>\n      <td>xxmaj</td>\n      <td>then</td>\n      <td>we</td>\n    </tr>\n    <tr>\n      <td>will</td>\n      <td>study</td>\n      <td>how</td>\n      <td>we</td>\n      <td>build</td>\n      <td>a</td>\n      <td>language</td>\n      <td>model</td>\n      <td>and</td>\n      <td>train</td>\n      <td>it</td>\n      <td>for</td>\n      <td>a</td>\n      <td>while</td>\n      <td>.</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#hide_input\n",
    "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
    "tokens = tkn(stream)\n",
    "bs,seq_len = 6,15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "source": [
    "看起来很整齐，如果这就是我们全部的数据，这样做当然没问题，但是对于一些大批量的数据，这样做问题就来了，即使我们使用更大一些的batch，比如64行，这样行内的元素可能有百万行。GPU就可能直呼受不住。所以我们需要更巧妙的方式。就像下面三个矩阵。\n",
    "\n",
    "矩阵1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <tbody>\n    <tr>\n      <td>xxbos</td>\n      <td>xxmaj</td>\n      <td>in</td>\n      <td>this</td>\n      <td>chapter</td>\n    </tr>\n    <tr>\n      <td>movie</td>\n      <td>reviews</td>\n      <td>we</td>\n      <td>studied</td>\n      <td>in</td>\n    </tr>\n    <tr>\n      <td>first</td>\n      <td>we</td>\n      <td>will</td>\n      <td>look</td>\n      <td>at</td>\n    </tr>\n    <tr>\n      <td>how</td>\n      <td>to</td>\n      <td>customize</td>\n      <td>it</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <td>of</td>\n      <td>the</td>\n      <td>preprocessor</td>\n      <td>used</td>\n      <td>in</td>\n    </tr>\n    <tr>\n      <td>will</td>\n      <td>study</td>\n      <td>how</td>\n      <td>we</td>\n      <td>build</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#hide_input\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "source": [
    "矩阵2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <tbody>\n    <tr>\n      <td>,</td>\n      <td>we</td>\n      <td>will</td>\n      <td>go</td>\n      <td>back</td>\n    </tr>\n    <tr>\n      <td>chapter</td>\n      <td>1</td>\n      <td>and</td>\n      <td>dig</td>\n      <td>deeper</td>\n    </tr>\n    <tr>\n      <td>the</td>\n      <td>processing</td>\n      <td>steps</td>\n      <td>necessary</td>\n      <td>to</td>\n    </tr>\n    <tr>\n      <td>xxmaj</td>\n      <td>by</td>\n      <td>doing</td>\n      <td>this</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <td>the</td>\n      <td>data</td>\n      <td>block</td>\n      <td>xxup</td>\n      <td>api</td>\n    </tr>\n    <tr>\n      <td>a</td>\n      <td>language</td>\n      <td>model</td>\n      <td>and</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#hide_input\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "source": [
    "矩阵3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <tbody>\n    <tr>\n      <td>over</td>\n      <td>the</td>\n      <td>example</td>\n      <td>of</td>\n      <td>classifying</td>\n    </tr>\n    <tr>\n      <td>under</td>\n      <td>the</td>\n      <td>surface</td>\n      <td>.</td>\n      <td>xxmaj</td>\n    </tr>\n    <tr>\n      <td>convert</td>\n      <td>text</td>\n      <td>into</td>\n      <td>numbers</td>\n      <td>and</td>\n    </tr>\n    <tr>\n      <td>we</td>\n      <td>'ll</td>\n      <td>have</td>\n      <td>another</td>\n      <td>example</td>\n    </tr>\n    <tr>\n      <td>.</td>\n      <td>\\n</td>\n      <td>xxmaj</td>\n      <td>then</td>\n      <td>we</td>\n    </tr>\n    <tr>\n      <td>it</td>\n      <td>for</td>\n      <td>a</td>\n      <td>while</td>\n      <td>.</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#hide_input\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "source": [
    "我们可以看到一个规律，这个6*15的大矩阵成功被我们分成了3个小矩阵，矩阵之间的规律是第一个矩阵的第一行->第二个矩阵的第一行->第三个矩阵的第一行->第一个矩阵的第二行->第二个矩阵的第二行->······，这样我们就可以将大的矩阵拆成很多个小的批次了。这么复杂的事情肯定库已经帮你做好了。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums200 = toks200.map(num)\n",
    "dl = LMDataLoader(nums200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((64, 72), (64, 72))"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "source": [
    "这个数据是前200个电影中的评论，将做好序数化的数据加载到`LMDataLoader`中就可以得到整理好的小batch了，默认的大小是64行*72列。\n",
    "\n",
    "另外因变量是自变量的后一个单词组成的矩阵，比如说："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "自变量："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'xxbos xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary .'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "source": [
    "因变量："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary . xxmaj'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "source": [
    "## 使用fastai的方法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "fastai会将这些工作都封装好了，当TextBlock被传入DataBlock的时候，令牌化和数字化的工作就会被自动的完成。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)"
   ]
  },
  {
   "source": [
    "完成之后可以调用show_batch函数来查看批次"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xxbos xxmaj africa xxmaj screams , one of the least seen of abbott&amp;costello 's films was an independent production that was released through xxmaj united xxmaj artists . xxmaj the thin plot has xxmaj hillary xxmaj brooke believing xxmaj costello has the map to a hidden territory that is rich with diamonds . xxmaj bud and xxmaj lou go to xxmaj africa at her behest with her two companions , the fighting xxmaj baer xxmaj brothers . xxmaj of course</td>\n      <td>xxmaj africa xxmaj screams , one of the least seen of abbott&amp;costello 's films was an independent production that was released through xxmaj united xxmaj artists . xxmaj the thin plot has xxmaj hillary xxmaj brooke believing xxmaj costello has the map to a hidden territory that is rich with diamonds . xxmaj bud and xxmaj lou go to xxmaj africa at her behest with her two companions , the fighting xxmaj baer xxmaj brothers . xxmaj of course the</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>got the balls to make the xxmaj christians out to be the intolerant , xenophobic and reactionary half - wits . \\n\\n xxmaj moral xxmaj orel is still an interesting watch ( as long as it comes between superior shows on xxmaj adult xxmaj swim ) because it is a satire . xxmaj however , xxmaj it is more a satire on the people that make it rather then the people it is depicting . \\n\\n xxmaj if you ever</td>\n      <td>the balls to make the xxmaj christians out to be the intolerant , xenophobic and reactionary half - wits . \\n\\n xxmaj moral xxmaj orel is still an interesting watch ( as long as it comes between superior shows on xxmaj adult xxmaj swim ) because it is a satire . xxmaj however , xxmaj it is more a satire on the people that make it rather then the people it is depicting . \\n\\n xxmaj if you ever want</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "source": [
    "接下来要做的就是对于模型进行微调了"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3,        #AWD_LSTM是一个经过预训练的模型，就和rnet34是一样的概念，drop_mult是\n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "source": [
    "learn.fit_one_cycle(1, 2e-2)    #进行一轮微调\n",
    "#电脑不行了跑不动了"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "训练完成之后，可以将模型储存下来"
   ]
  }
 ]
}