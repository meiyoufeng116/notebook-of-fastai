{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 第九章 NPL 自然语言处理和分词"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "source": [
    "NPL （Natural Language Processing）自然语言处理。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 文本预处理\n",
    "首先我们要做的是预测文本的下一个单词，在建立神经网络模型之前，对于文本中的单词，预处理主要有以下步骤：\n",
    "    \n",
    "1. 将所有出现的单词编译成可编程变量，将其列到一个列表中（这个list被成为vocab）\n",
    "2. 给每个单词附上独特的编号\n",
    "3. 建立一个嵌入矩阵来放这些单词，每一行对应一个vocab\n",
    "4. 使用这些矩阵作为神经网络的第一层\n",
    "\n",
    "对于文本数据，首先我们要把要预测的文本变成一个长长的字符串，然后模型的自变量应该是这串长长字符串的第一个单词到最后第二个单词，因变量是第二个单词到最后一个单词。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- 标记化：将文本转换为单词（或字符或子字符串，取决于模型的粒度）列表\n",
    "- 数字化：列出出现的所有唯一词（词汇），并通过在词汇中查找其索引将每个单词转换为数字\n",
    "- 语言模型数据加载器的创建：fastai提供了一个“ LMDataLoader”类，该类自动处理创建因变量而该因变量偏离自变量一个标记的情况。它还处理一些重要的细节，例如，如何以因变量和自变量按需保持其结构的方式对训练数据进行混洗\n",
    "- 语言模型的创建：我们需要一种特殊的模型，该模型可以完成我们之前从未见过的事情：处理输入列表，可以任意大小。有很多方法可以做到这一点。在本章中，我们将使用“递归神经网络”（RNN）。我们将在<< chapter_nlp_dive >>中获得这些RNN的详细信息，但是现在，您可以将其视为另一个深度神经网络。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 令牌化\n",
    "\n",
    "在这个阶段主要处理的是如何将一个具有实际意义的词挑出来。遇到的问题会有很多，比如don’t是一个词还算是两个词呢？有些很长的化学名词和生物名词怎么办？如何处理中文和日语这样没有具体单词划分的语言？这些问题的解决要基于三个原则：\n",
    "\n",
    "- 基于单词的分割：在词和词之间的空格出进行分割，同时也要将有语义不同的地方分割出来，比如说dont会分成do nt,这个情况下标点符号一般是分割的地方。_\n",
    "- 基于子词的分割：根据最常出现的子字符串来进行分割，比如occasion可能会被分割成o c ca sion\n",
    "- 基于字母的分割：单个字母单个字母进行分割\n",
    "\n",
    "fastai没有做自己的分词器，但是做了一个接口可以调用不同的分词器\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A new version of this dataset is available, downloading...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'C:\\\\Users\\\\94323\\\\.fastai\\\\archive\\\\imdb.tgz'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ffe2dce53cf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muntar_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURLs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMDB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\data\\external.py\u001b[0m in \u001b[0;36muntar_data\u001b[1;34m(url, fname, dest, c_key, force_download, extract_func, timeout)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mforce_download\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_try_from_storage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mURLs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'storage'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'C:\\\\Users\\\\94323\\\\.fastai\\\\archive\\\\imdb.tgz'"
     ]
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  }
 ]
}